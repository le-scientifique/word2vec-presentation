{
 "metadata": {
  "name": "",
  "signature": "sha256:40d6589b3714ef0f9b71d45a3bac9f37360316b44b235303e8fc72d1bd8cc66c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using Word2Vec to extract features from Legal Corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import os\n",
      "import re\n",
      "import glob\n",
      "import nltk\n",
      "from gensim.models import word2vec # pip install --upgrade gensim\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "tokenizer = RegexpTokenizer(r'\\w[\\w\\-\\'\\.]+')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "source_files_dir = \"/home/harsha/le-scientifique/word2vec-presentation/legal-corpus-50\"\n",
      "root_dir = \"/home/harsha/le-scientifique/word2vec-presentation\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_corpus(src_dir,dest_dir):\n",
      "\t''' Each sentence as a line '''\n",
      "\tfiles = glob.glob(os.path.join(src_dir,\"*\"))\n",
      "\tsentences = []\n",
      "\twith open(os.path.join(dest_dir,\"corpus.txt\"),\"w\") as fobj:\n",
      "\t\tfor f in files:\n",
      "\t\t\tfor line in open(f).readlines():\n",
      "\t\t\t\tline = re.sub(r'[\\ \\t]+',' ',line)\n",
      "\t\t\t\tline = re.sub(r'([\\.\\-])+',r'\\1',line)\n",
      "\t\t\t\tif line.strip(\"[\\n\\. ]\") != \"\" and len(tokenizer.tokenize(line))>1:\n",
      "\t\t\t\t\tfobj.write(line.strip()+'\\n')\n",
      "\tprint \"Training corpus file created at %s/corpus.txt\" %dest_dir\n",
      "\treturn os.path.join(dest_dir,\"corpus.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_file = create_corpus(source_files_dir,root_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training corpus file created at /home/harsha/le-scientifique/word2vec-presentation/corpus.txt\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Training the model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n",
      "\n",
      "* Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
      "* Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
      "* Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
      "* Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
      "* Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
      "* Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
      "* Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set values for various parameters\n",
      "num_features = 300    # Word vector dimensionality                      \n",
      "min_word_count = 40   # Minimum word count                        \n",
      "num_workers = 4       # Number of threads to run in parallel\n",
      "context = 10          # Context window size                                                                                    \n",
      "downsampling = 1e-3   # Downsample setting for frequent words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = open(corpus_file).read().split('\\n')\n",
      "print len(data)\n",
      "sentences = []\n",
      "for sent in data:\n",
      "    a = [w.strip('[\\.\\,\\:\\)\\(\\[\\]\\{\\}]') for w in tokenizer.tokenize(sent) if w.strip('[\\.\\,\\:\\)\\(\\[\\]\\{\\}]') != \"\"]\n",
      "    sentences.append(a) # append a list of words from a sentence by keeping the word order\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "105933\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "# Initialize and train the model (this will take some time)\n",
      "print(\"Training model...\")\n",
      "model = Word2Vec(sentences, workers=num_workers, \\\n",
      "            size=num_features, min_count = min_word_count, \\\n",
      "            window = context, sample = downsampling)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training model...\n",
        "CPU times: user 22.3 s, sys: 249 ms, total: 22.6 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 6.46 s\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# If you don't plan to train the model any further, calling \n",
      "# init_sims will make the model much more memory-efficient.\n",
      "model.init_sims(replace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# It can be helpful to create a meaningful model name and \n",
      "# save the model for later use. You can load it later using Word2Vec.load()\n",
      "model_name = \"300features_40minwords_10context\"\n",
      "model.save(model_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exploring the Model Results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      ">>> model.most_similar(\"liability\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "[('damages', 0.9264083504676819),\n",
        " ('improvements', 0.9176770448684692),\n",
        " ('claims', 0.8995101451873779),\n",
        " ('property', 0.893409013748169),\n",
        " ('alterations', 0.8869436383247375),\n",
        " ('tenant', 0.880409836769104),\n",
        " ('repairs', 0.8779527544975281),\n",
        " ('estate', 0.861427366733551),\n",
        " ('repair', 0.8585595488548279),\n",
        " ('installation', 0.8514100313186646)]"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      ">>> model.most_similar(\"expiration\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "[('termination', 0.9012417197227478),\n",
        " ('extension', 0.7780113220214844),\n",
        " ('terminate', 0.752634584903717),\n",
        " ('date', 0.7298182845115662),\n",
        " ('end', 0.7230787873268127),\n",
        " ('anniversary', 0.718147873878479),\n",
        " ('commencement', 0.711777925491333),\n",
        " ('issuance', 0.7102537155151367),\n",
        " ('Date', 0.6966657638549805),\n",
        " ('separation', 0.6941319704055786)]"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      ">>> model.most_similar(\"party\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[('arbitrator', 0.6564234495162964),\n",
        " ('hearing', 0.6333960294723511),\n",
        " ('consent', 0.63277268409729),\n",
        " ('Landlord', 0.627322793006897),\n",
        " ('arbitration', 0.6118891835212708),\n",
        " ('statement', 0.6084203124046326),\n",
        " ('Owner', 0.6040895581245422),\n",
        " ('parties', 0.6013562679290771),\n",
        " ('UAW', 0.5960632562637329),\n",
        " ('mutually', 0.5851148962974548)]"
       ]
      }
     ],
     "prompt_number": 36
    }
   ],
   "metadata": {}
  }
 ]
}